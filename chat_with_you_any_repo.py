# -*- coding: utf-8 -*-
"""chat with you repo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-NX3K4NVvg8oIA_in4pRZyk61PzjRfu0
"""

! pip install -qU "langchain[mistralai]" -q
! pip install -U langchain-community -q
!pip install gitpython -q
! pip install --upgrade langchain -q
! pip install langchain-deeplake deeplake -q
! pip install -qU langchain-mistralai -q
! pip install langchain faiss-cpu -q

from langchain.document_loaders import GitLoader
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    Language
)
from langchain.prompts import (
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  SystemMessagePromptTemplate,
)

from langchain.chains.question_answering import load_qa_chain
from typing import List
from langchain.schema import Document
from langchain_community.vectorstores import DeepLake
from langchain_mistralai import MistralAIEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import FAISS

import getpass
import os

if not os.environ.get("MISTRAL_API_KEY"):
  os.environ["MISTRAL_API_KEY"] = getpass.getpass("Enter API key for Mistral AI: ")

from langchain.chat_models import init_chat_model

model = init_chat_model("mistral-large-latest", model_provider="mistralai")

"""# load_Github_Content"""

def load_github_repo(repo_url: str, branch: str = "main", repo_path:str ="./repo") -> list:
  """
    Load the contents of a GitHub repository using LangChain's GitLoader.

    Parameters:
    - repo_url: str, GitHub URL of the repository (e.g. 'https://github.com/username/repo')
    - branch: str, the branch to clone (default is 'main')
    - repo_path: str, local path to clone the repository (default is './repo')

    Returns:
    - documents: list of Document objects loaded from the repository
    """

  loader = GitLoader(
        clone_url=repo_url,
        repo_path=repo_path,
        branch=branch
    )

  document = loader.load()

  return document

repo_url = "https://github.com/AhemdMahmoud/lora-sft-gemma-3-4b-"
docs = load_github_repo(repo_url)

len(docs)

print(docs[2].page_content)

docs[2].metadata["source"]

os.path.splitext(docs[2].metadata["source"])

os.path.splitext(docs[2].metadata["source"])[1]

"""# Language from Extention  from scratch"""

def get_language_from_extension(ext):
    ext_to_lang = {
        ".cpp": Language.CPP,
        ".go": Language.PYTHON,
        ".java": Language.JAVA,
        ".js": Language.JS,
        ".jsx": Language.JS,
        ".ts": Language.JS,
        ".tsx": Language.JS,
        ".php": Language.PHP,
        ".proto": Language.PROTO,
        ".py": Language.PYTHON,
        ".rst": Language.RST,
        ".rb": Language.RUBY,
        ".rs": Language.RUST,
        ".scala": Language.SCALA,
        ".swift": Language.SWIFT,
        ".md": Language.MARKDOWN,
        ".tex": Language.LATEX,
        ".html": Language.HTML,
        ".htm": Language.HTML,
        ".sol": Language.SOL,
        ".css": Language.HTML,
        ".txt": Language.MARKDOWN,
        ".json": Language.MARKDOWN,
        # ".ipynb": None,  # skip notebooks
        ".ipynb":Language.MARKDOWN
    }
    return ext_to_lang.get(ext.lower(), Language.MARKDOWN)

"""# split docs"""

Language.PYTHON

Language.MARKDOWN

"""# ‚úÖ It splits based on code structure (functions, classes) whenever possible ‚Äî not just character count."""

def split_documents(documents: List[Document], chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Document]:
    """
    Split documents into smaller chunks using RecursiveCharacterTextSplitter.

    Parameters:
    - documents: List of LangChain Document objects.
    - chunk_size: The maximum size of each text chunk.
    - chunk_overlap: The number of overlapping characters between chunks.

    Returns:
    - List of split Document objects.
    """
    split_documents_list = []
    for doc in documents:
        try:
            ext = os.path.splitext(doc.metadata["source"])[1]
            lang = get_language_from_extension(ext)
            if lang is None:
              print(f"Skipping unsupported file type: {ext}")
              continue


            text_splitter = RecursiveCharacterTextSplitter.from_language(
                language=lang,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )

            split_docs = text_splitter.create_documents([doc.page_content])

            for split_doc in split_docs:
                split_doc.metadata = doc.metadata  # safely copy metadata
                split_documents_list.append(split_doc)

        except Exception as e:
            print(f"Error splitting document: {doc.metadata['source']}, Exception: {str(e)}")

    return split_documents_list

"""# vectorstore"""

def store_in_faiss(documents, faiss_index_path: str):
  """
    Store documents into FAISS vector store.

    Parameters:
    - documents: List of LangChain Document objects.
    - faiss_index_path: Path where FAISS index will be saved.

    Returns:
    - vectorstore: The created FAISS vector store object.
    """


  embeddings = MistralAIEmbeddings(model="mistral-embed")
  vectorstore = FAISS.from_documents(documents, embeddings)
  vectorstore.save_local(faiss_index_path)
  return vectorstore

def load_conversation_chain(vectorstore):
  # model = init_chat_model("mistral-large-latest", model_provider="mistralai")
  model = model
  memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
  general_system_template = """You are a superintelligent AI that answers questions about codebases.
    You are:
    - helpful & friendly
    - good at answering complex questions in simple language
    - an expert in all programming languages
    - able to infer the intent of the user's question

    The user will ask a question about their codebase, and you will answer it.

    When the user asks their question, you will answer it by searching the codebase for the answer.
    Answer the question using the code file(s) below:
    ----------------
        {context}"""
  # Define your user message template
  general_user_template = "Question:```{question}```"

  system_message_prompt = SystemMessagePromptTemplate.from_template(general_system_template)
  user_message_prompt = HumanMessagePromptTemplate.from_template(general_user_template)

  qa_prompt = ChatPromptTemplate.from_messages([system_message_prompt,user_message_prompt])


  conversation_chain= ConversationalRetrievalChain.from_llm(
      llm= model ,
      retriever= vectorstore.as_retriever(),
      memory=memory,
      combine_docs_chain_kwargs={"prompt": qa_prompt}
  )
  return conversation_chain

import gradio as gr

def chat_with_repo(repo_url, query):
    try:
        # Load documents
        docs = load_github_repo(repo_url)
        if not docs:
            return "‚ùå Could not load documents from the provided repository URL."

        # Split documents
        split_documents_list = split_documents(docs)
        if not split_documents_list:
             return "‚ùå Could not split documents. Check if supported file types are present."

        # Create vector store (using a temporary path for demonstration)
        faiss_index_path = "./faiss_index_temp"
        vectorstore = store_in_faiss(split_documents_list, faiss_index_path)

        # Create conversation chain
        chain = load_conversation_chain(vectorstore)

        # Get response from the chain
        response = chain({"question": query})
        answer = response["answer"]

        # Beautify output with Markdown
        formatted_answer = f"### üí° Answer\n\n{answer}"

        return formatted_answer

    except Exception as e:
        return f"‚ùå An error occurred:\n\n```\n{str(e)}\n```"

# Gradio interface with Markdown output
iface = gr.Interface(
    fn=chat_with_repo,
    inputs=[
        gr.Textbox(label="GitHub Repository URL"),
        gr.Textbox(label="Your Question")
    ],
    outputs=gr.Markdown(),
    title="üß† Codebase Chatbot",
    description="Enter a GitHub repository URL and ask questions about its code."
)

iface.launch(debug=True)

